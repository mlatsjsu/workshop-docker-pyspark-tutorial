{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial of PySpark on Titanic Dataset\n",
    "\n",
    "Apache Spark is an open-source distributed framework that is used for Big Data analysis. \n",
    "Pyspark is the Python API for Apache Spark, equipped with libraries such as PySparkSQL and MLlib.\n",
    "\n",
    "<img src=\"images/distributed-computing-with-spark-7-638.jpg\" alt=\"distributed computing image\" width =\"500\"/>\n",
    "<img src=\"images/pyspark-overview.png\" alt=\"pyspark overview image\" width =\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfkkTzOw7hy1",
    "outputId": "cc71a179-371c-4ac5-ff10-aa7f5a7bfc9e"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "spark = SparkSession.builder.appName('titanic_logreg').getOrCreate()\n",
    "df = spark.read.csv('titanic.csv', inferSchema = True, header = True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m55CRrw_7hy_",
    "outputId": "39b73d05-4216-45ae-af4a-ac2bd2b37c19",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3qw7vWd7hzF",
    "outputId": "74b0c765-5fae-442b-d357-538ddc27ad93",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = df.toPandas()\n",
    "titanic_df.sample(5)\n",
    "#note:\n",
    "#\"sibsp\" == Number of Siblings/Spouses Aboard\n",
    "#\"parch\" == Number of Parents/Children Aboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TEw96aj7hzL"
   },
   "outputs": [],
   "source": [
    "# Drop PassengerId, Cabin, Ticket\n",
    "my_col = df.select(['Survived','Pclass','Sex','Age','SibSp','Parch','Fare','Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wM-Q6TZx7hzS"
   },
   "outputs": [],
   "source": [
    "final_data = my_col.na.drop()\n",
    "final_data.toPandas().info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data for Insights\n",
    "\n",
    "Visually plotting a data category (eg, passenger age) as it relates to categorical data (eg, pclass = passenger class) may give insights into how data may be correlated.  \n",
    "\n",
    "In the graph below, we see the age distributions of passengers in the different fare classes (1st, 2nd, 3rd), where passenger class may be taken as a proxy for socio-economic class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for x in [1,2,3]:    ## for 3 classes\n",
    "    final_data.toPandas().Age[final_data.toPandas().Pclass == x].plot(kind=\"kde\")\n",
    "plt.title(\"Age wrt Pclass\")\n",
    "plt.legend((\"1st\",\"2nd\",\"3rd\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Deeper Dive into Data Distributions and Correlations\n",
    "\n",
    "The titanic_df.describe() call, above, provided a quick view of statistical descriptors of each data type.  For a deeper dive, the pandas-profiling .profile_report() call, below, provides further information, such as visual summaries of distributions, plotting of one data interactions, the highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.toPandas().profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder\n",
    "\n",
    "A <b>transformer</b> converts one data frame to another, oftentimes appending or combining columns.The transformers below are considered <b>feature transformers</b>, as they transform specific features in the data frame instead of the whole dataframe itself.\n",
    "\n",
    "* <b>VectorAssembler</b>: combines a given list of columns into one single vector. Often used to combine raw features and features generated by other feature transformers into one feature vector. \n",
    "* <b>StringIndexer:</b> encodes a string column of labels to a column of label indices\n",
    "* <b>VectorIndexer:</b> takes an input column in vector form, decides which values are categorical and changes those values to indices. This is often used for Decision Trees and Tree Ensembles.\n",
    "* <b>OneHotEncoder:</b> maps a categorical feature, represented as a label index, to a binary vector. Each binary entry in that vector indicates the presence of a certain feature value out of the all the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-r9ebKU7hzX"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder)\n",
    "\n",
    "gender_indexer = StringIndexer(inputCol = 'Sex', outputCol = 'SexIndex')\n",
    "gender_encoder = OneHotEncoder(inputCol='SexIndex', outputCol = 'SexVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGsDurkG7hzc"
   },
   "outputs": [],
   "source": [
    "embark_indexer = StringIndexer(inputCol = 'Embarked', outputCol = 'EmbarkIndex')\n",
    "embark_encoder = OneHotEncoder(inputCol = 'EmbarkIndex', outputCol = 'EmbarkVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jdxi2s1i7hzh"
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = ['Pclass', 'SexVec', 'Age', 'SibSp', 'Parch', 'Fare', 'EmbarkVec'], outputCol = 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Pipeline?\n",
    "\n",
    "Spark represents the machine learning workflow using functions called transformers and estimators, located within a pipeline that links these functions together. \n",
    "\n",
    "As mentioned earlier, a <b>transformer</b> converts one data frame to another, oftentimes appending or combining columns. An <b>estimator</b> is an algorithm that fits or trains  data in some way, producing a model for that data.\n",
    "\n",
    "A <b>pipeline</b> is a series of pipeline stages linked together. Pipeline stages are either transformers and estimators. Our pipeline below consists of 6 stages. It links the feature transformers of gender_indexer, embark_indexer, gender_encoder, embark_encoder and assembler, with our estimator, log_reg.\n",
    "\n",
    "<img src=\"images/ml-Pipeline.png\" alt= \"pipeline image\" width=\"500\">\n",
    "<img src=\"images/ml-PipelineModel.png\" alt=\"pipelinemodel image\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Y5wBojJ7hzn"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "log_reg = LogisticRegression(featuresCol = 'features', labelCol = 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvPLb5fy7hzs"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [gender_indexer, embark_indexer, \n",
    "                             gender_encoder, embark_encoder,\n",
    "                             assembler, log_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1m82Ovp67hzw"
   },
   "outputs": [],
   "source": [
    "train, test = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Estimator class has a <b>fit()</b> function which is called on the train data frame, passing that data through the stages of our pipeline. The fit() function trains and creates a linear regression model (we call it fit_model) that we can now pass data into, in order to make predictions as to whether a passenger died or survived. \n",
    "\n",
    "Our fitted pipeline, fit_model, is a <b>transformer</b>. It transforms data frames of passenger information into dataframes that include predictions by calling the Transformer class' transform() function on our test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VEcSEk27hz0"
   },
   "outputs": [],
   "source": [
    "fit_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBXflO_N7hz3"
   },
   "outputs": [],
   "source": [
    "results = fit_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8R-iwdts7hz8",
    "outputId": "78433989-a5f6-4071-8c7d-738cec2176ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.select('prediction', 'Survived').toPandas().sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "After the model is trained and tested, one way the results can be explored is through different types of plots that emphasize predictions per data type, like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g = sns.FacetGrid(results.toPandas(), col=\"Sex\", row=\"Survived\", margin_titles=True)\n",
    "g.map(plt.hist, \"prediction\",color=\"purple\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxcUjMbt7h0B",
    "outputId": "af754cea-6b4e-4c91-9d30-7cbfc1628855"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "eval = BinaryClassificationEvaluator(rawPredictionCol = 'rawPrediction', labelCol = 'Survived')\n",
    "AUC = eval.evaluate(results)\n",
    "AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, for example, display aggregate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(results.toPandas(),col='Survived')\n",
    "g = g.map(sns.distplot,'prediction') #was age"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Logistic_Regression_Titanic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
